{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bangla_news_headline_classification_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r-gjd5-0VlCG"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9GboruKm_rW"
      },
      "source": [
        "# install modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_R157PjMIFc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# !pip install pytorch-lightning==1.4.9\n",
        "!pip install -q pytorch-lightning wandb\n",
        "!pip install gensim --upgrade\n",
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qVBJO_31PRk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re,nltk,json\n",
        "import random\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "SEED = 1234\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "from tqdm.notebook import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK2oKGoto1iC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from pytorch_lightning import LightningModule,LightningDataModule,Trainer\n",
        "from torchmetrics.functional import accuracy\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor,ModelCheckpoint,EarlyStopping,ProgressBar\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIqpm34x2rms"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_metric\n",
        "import transformers\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        "    get_constant_schedule_with_warmup\n",
        ")\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeZgRup520II"
      },
      "outputs": [],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g08gf86E7_6J"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsSrVuUA8ge-"
      },
      "source": [
        "# mount gsuit drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktAc6JEZuDyP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training on potrika dataset"
      ],
      "metadata": {
        "id": "3bLCJSf0OaSt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-gjd5-0VlCG"
      },
      "source": [
        "## create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf5G2ngWbWUu"
      },
      "outputs": [],
      "source": [
        "Economy_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/Economy_40k.csv',usecols=[1,2])\n",
        "Education_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/Education_40k.csv',usecols=[1,2])\n",
        "Entertainment_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/Entertainment_40k.csv',usecols=[1,2])\n",
        "International_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/International_40k.csv',usecols=[1,2])\n",
        "politics_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/politics_40k.csv',usecols=[1,2])\n",
        "National_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/National_40k.csv',usecols=[1,2])\n",
        "ScienceTechnology_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/ScienceTechnology_40k.csv',usecols=[1,2])\n",
        "Sports_40k = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/BalancedDataset/Sports_40k.csv',usecols=[1,2])\n",
        "Economy_40k.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA9fDoS_cQyb"
      },
      "outputs": [],
      "source": [
        "all_combined = pd.concat([Economy_40k,Education_40k,Entertainment_40k,International_40k,politics_40k,\n",
        "                          National_40k,ScienceTechnology_40k,Sports_40k],axis = 0)\n",
        "print(Economy_40k.shape,Education_40k.shape,Entertainment_40k.shape,International_40k.shape,politics_40k.shape,\n",
        "                          National_40k.shape,ScienceTechnology_40k.shape,Sports_40k.shape,all_combined.shape)\n",
        "all_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtinJkB4df_J"
      },
      "outputs": [],
      "source": [
        "all_combined.article.iloc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWDxBUaLKaM1"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "def clean_titles(title):\n",
        "  title =str(title)\n",
        "  while re.search('[\\u0980-\\u09ff][\\'\\’\\‘\\”\\“\\,\\\\.]+[\\u0980-\\u09ff]', title):\n",
        "      pos = re.search('[\\u0980-\\u09ff][\\'\\’\\‘\\”\\“\\,\\\\.]+[\\u0980-\\u09ff]', title).start()\n",
        "      title = title[:pos+1] + title[pos+2:]\n",
        "  title = re.sub(r\"[\\’\\‘\\”\\“]+\", \"'\", title)\n",
        "  title = re.sub(r\"[\\*\\#\\;]+\", \"\", title)\n",
        "  title = re.sub(r\"[\\n\\r]+\", \"\", title)\n",
        "  title = re.sub(r'আরো পড়ুন.*','',title,flags=re.U|re.S) \n",
        "  return title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xyAs-OLI9Up"
      },
      "outputs": [],
      "source": [
        "all_combined = all_combined.drop_duplicates(['article']) \n",
        "all_combined['article'] = all_combined['article'].apply(clean_titles) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7WSPSLle1gS"
      },
      "outputs": [],
      "source": [
        "all_combined['article_length'] = all_combined['article'].apply(lambda x:len(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY8mJE5amnrf"
      },
      "outputs": [],
      "source": [
        "all_combined.loc[all_combined.article_length<10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmfKT0M6mbBe"
      },
      "outputs": [],
      "source": [
        "c = collections.Counter(all_combined['article_length'].values)\n",
        "sorted(c.items(),key = lambda x:x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAhiR0Llmvgg"
      },
      "outputs": [],
      "source": [
        "all_combined = all_combined.loc[all_combined.article_length>5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3me4rPYJm2Y4"
      },
      "outputs": [],
      "source": [
        "all_combined.to_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/potrika_all_combined.csv',index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg8YT6Ns8RcK"
      },
      "outputs": [],
      "source": [
        "all_combined.iloc[:1000,:].to_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/potrika_all_combined_mini.csv',index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3owpvmVoYH"
      },
      "source": [
        "## load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxqgaYf0n2gR"
      },
      "outputs": [],
      "source": [
        "all_combined = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/potrika_all_combined.csv')\n",
        "all_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvEs6c_pK68T"
      },
      "outputs": [],
      "source": [
        "all_combined.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foJiVkUioJCL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test=train_test_split(all_combined,stratify = all_combined['class'].values,shuffle=True,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfIoIm6-LbJu"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtynv5VSLcrK"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8txtAPGzOUGy"
      },
      "outputs": [],
      "source": [
        "train.to_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/train.csv',index=None)\n",
        "test.to_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/test.csv',index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg56zXHbY7hJ"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg_u_U8cZFLa"
      },
      "outputs": [],
      "source": [
        "train.shape,test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MkGHl5bKDyX"
      },
      "outputs": [],
      "source": [
        "label_list = all_combined['class'].unique()\n",
        "label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzDNLcmVqgT"
      },
      "source": [
        "## dataloader class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkibnDLOKap3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import XLMRobertaTokenizer,BertTokenizerFast,BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GhUKeslZyZo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcynVNb67_6K"
      },
      "outputs": [],
      "source": [
        "all_checkpoints=['bert-base-multilingual-cased',\n",
        "                 'sagorsarker/bangla-bert-base',\n",
        "                 'neuralspace-reverie/indic-transformers-bn-bert',\n",
        "                 'neuralspace-reverie/indic-transformers-bn-roberta',\n",
        "                 'distilbert-base-multilingual-cased',\n",
        "                 'neuralspace-reverie/indic-transformers-bn-distilbert',\n",
        "                 'monsoon-nlp/bangla-electra',\n",
        "                 'csebuetnlp/banglabert',\n",
        "                 'neuralspace-reverie/indic-transformers-bn-xlmroberta'\n",
        "                 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqpZNOXC53rr"
      },
      "outputs": [],
      "source": [
        "tokenizer_list=[]\n",
        "tokenizer_list.append('AutoTokenizer.from_pretrained(all_checkpoints[0])')\n",
        "tokenizer_list.append('BertTokenizer.from_pretrained(all_checkpoints[1])')\n",
        "tokenizer_list.append('BertTokenizer.from_pretrained(all_checkpoints[2])')\n",
        "tokenizer_list.append('AutoTokenizer.from_pretrained(all_checkpoints[3])')\n",
        "tokenizer_list.append('AutoTokenizer.from_pretrained(all_checkpoints[4])')\n",
        "tokenizer_list.append('AutoTokenizer.from_pretrained(all_checkpoints[5])')\n",
        "tokenizer_list.append('AutoTokenizer.from_pretrained(all_checkpoints[6])')\n",
        "tokenizer_list.append('AutoTokenizer.from_pretrained(all_checkpoints[7])')\n",
        "tokenizer_list.append('XLMRobertaTokenizer.from_pretrained(all_checkpoints[8])')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIIcgWncJ-n_"
      },
      "outputs": [],
      "source": [
        "class MNISTDataModule(LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      batch_size: int = 64,\n",
        "      num_workers: int = 4,\n",
        "      label_list=[0,1,2],\n",
        "      model_number = 7,\n",
        "      max_seq_length=64,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.num_workers = num_workers  \n",
        "      self.label_list = label_list\n",
        "      self.num_classes = len(self.label_list)\n",
        "      self.model_name =all_checkpoints[model_number]\n",
        "      self.max_seq_length = max_seq_length\n",
        "      self.tokenizer = eval(tokenizer_list[model_number])\n",
        "      self.traindf = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/train.csv')\n",
        "      self.traindf = self.traindf.sample(frac=1,random_state=42).reset_index(drop=True)\n",
        "      self.testdf = pd.read_csv('/content/drive/MyDrive/machine_learning/Potrika Dataset/test.csv')\n",
        "      self.testdf= self.testdf.sample(frac=1,random_state=42).reset_index(drop=True)\n",
        "      self.label_map = {}\n",
        "      for (i, label) in enumerate(self.label_list):\n",
        "        self.label_map[label] = i\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "      \n",
        "      #The labeled (test) dataset is assigned with a mask set to True\n",
        "      self.test_examples = list(zip(self.testdf['article'].values,self.testdf['class'].values))\n",
        "      self.train_examples = list(zip(self.traindf['article'].values,self.traindf['class'].values))\n",
        "      \n",
        "      \n",
        "  def generate_data_loader(self,input_examples,label_map, do_shuffle = False,train = True):\n",
        "    '''\n",
        "    Generate a Dataloader given the input examples, eventually masked if they are \n",
        "    to be considered NOT labeled.\n",
        "    '''\n",
        "    # Building the TensorDataset\n",
        "    if train and os.path.exists('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_input_ids.npy'):\n",
        "      input_ids = torch.from_numpy(np.load('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_input_ids.npy'))\n",
        "      input_mask_array = torch.from_numpy(np.load('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_input_mask_array.npy'))\n",
        "      label_id_array = torch.from_numpy(np.load('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_label_id_array.npy'))\n",
        "      \n",
        "    elif os.path.exists('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_input_ids.npy'):\n",
        "      input_ids = torch.from_numpy(np.load('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_input_ids.npy'))\n",
        "      input_mask_array = torch.from_numpy(np.load('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_input_mask_array.npy'))\n",
        "      label_id_array = torch.from_numpy(np.load('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_label_id_array.npy'))\n",
        "      \n",
        "    else:  \n",
        "      #-----------------------------------------------\n",
        "      # Generate input examples to the Transformer\n",
        "      #-----------------------------------------------\n",
        "      input_ids = []\n",
        "      input_mask_array = []\n",
        "      label_id_array = []\n",
        "\n",
        "      # Tokenization \n",
        "      for text,label in input_examples:\n",
        "        encoded_sent = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_seq_length, padding=\"max_length\", truncation=True)\n",
        "        input_ids.append(encoded_sent)\n",
        "        label_id_array.append(label_map[label])\n",
        "        \n",
        "      \n",
        "      # Attention to token (to ignore padded input wordpieces)\n",
        "      for sent in input_ids:\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "        input_mask_array.append(att_mask)\n",
        "      # Convertion to Tensor\n",
        "      input_ids = torch.tensor(input_ids) \n",
        "      input_mask_array = torch.tensor(input_mask_array)\n",
        "      label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "      \n",
        "      if train:\n",
        "        np.save('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_input_ids.npy',input_ids.numpy(),allow_pickle =False)\n",
        "        np.save('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_input_mask_array.npy',input_mask_array.numpy(),allow_pickle =False)\n",
        "        np.save('/content/drive/MyDrive/machine_learning/Potrika Dataset/train_label_id_array.npy',label_id_array.numpy(),allow_pickle =False)\n",
        "      else:\n",
        "        np.save('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_input_ids.npy',input_ids.numpy(),allow_pickle =False)\n",
        "        np.save('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_input_mask_array.npy',input_mask_array.numpy(),allow_pickle =False)\n",
        "        np.save('/content/drive/MyDrive/machine_learning/Potrika Dataset/test_label_id_array.npy',label_id_array.numpy(),allow_pickle =False)\n",
        "    dataset = TensorDataset(input_ids, input_mask_array, label_id_array)\n",
        "    if do_shuffle:\n",
        "      sampler = RandomSampler\n",
        "    else:\n",
        "      sampler = SequentialSampler\n",
        "\n",
        "    # Building the DataLoader\n",
        "    return DataLoader(\n",
        "                dataset,  # The training samples.\n",
        "                sampler = sampler(dataset), \n",
        "                batch_size = self.batch_size,\n",
        "                pin_memory = True,\n",
        "                num_workers=self.num_workers) # Trains with this batch size.\n",
        "\n",
        "  def train_dataloader(self):\n",
        "      return self.generate_data_loader(self.train_examples,\\\n",
        "                                  self.label_map, do_shuffle = True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "      return self.generate_data_loader(self.test_examples, self.label_map,\\\n",
        "                                  do_shuffle = False,train=False)\n",
        "\n",
        "  # def test_dataloader(self):\n",
        "  #     return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWvBYgIMOtoo"
      },
      "outputs": [],
      "source": [
        "gb_dataset = MNISTDataModule(model_number =7,label_list = label_list)\n",
        "len(gb_dataset.train_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNPwEasVOtop"
      },
      "outputs": [],
      "source": [
        "for batch in gb_dataset.train_dataloader():\n",
        "  print(batch)\n",
        "  print(batch[0].shape,batch[1].shape,batch[2].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLVUAAPhOtop"
      },
      "outputs": [],
      "source": [
        "for batch in gb_dataset.val_dataloader():\n",
        "  print(batch[0].shape,batch[1].shape,batch[2].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqKP9pTFOwJI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "sweep_config = {\n",
        "    'learning_rate': 1e-05,\n",
        "      'batch_size':64,\n",
        "      'warmup_proportion':0.1,\n",
        "      'num_train_examples': len(gb_dataset.train_examples),\n",
        "      'model_number': 7,\n",
        "      'epochs': 10,\n",
        "     'device': 'cuda'}\n",
        "sweep_config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "vH4Yu5iPhmOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzxVvEHmajmf"
      },
      "outputs": [],
      "source": [
        "class TransformerForSequenceClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified version of the same class by HuggingFace.\n",
        "    See transformers/modeling_distilbert.py in the transformers repository.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, pretrained_model_name: str, num_classes: int = 2, dropout: float = 0.5,\n",
        "        mean_pool: bool=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pretrained_model_name (str): HuggingFace model name.\n",
        "                See transformers/modeling_auto.py\n",
        "            num_classes (int): the number of class labels\n",
        "                in the classification task\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            pretrained_model_name, num_labels=num_classes\n",
        "        )\n",
        "        for k in config.to_dict().keys():\n",
        "          if 'dropout' in k and 'classifier' not in k:\n",
        "            config.update({k:0.3})\n",
        "\n",
        "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
        "        # self.model.encoder = reinit_autoencoder_model(self.model.encoder, reinit_num_layers=1)\n",
        "        # print('I am here in init method')\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mean_pool=mean_pool\n",
        "        self.num_labels = num_classes\n",
        "        if torch.cuda.is_available():\n",
        "          self.model.cuda()\n",
        "          self.classifier.cuda()\n",
        "          self.dropout.cuda()\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # print('I am in forward method')\n",
        "        output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        # print('I got output')\n",
        "        hidden_state = output[0]  # (bs, seq_len, dim)\n",
        "        outputs = (hidden_state,)\n",
        "        if not self.mean_pool:\n",
        "          pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        else:\n",
        "          pooled_output = hidden_state.mean(axis=1)  # (bs, dim)\n",
        "\n",
        "        # pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
        "        # pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        # print('I got output2')\n",
        "        # print(pooled_output.shape)\n",
        "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
        "        # print('I got output3')\n",
        "        # outputs = (logits,) + output[1:]\n",
        "        outputs = (logits,) + outputs\n",
        "        # print('labels:',labels)\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                loss_fct = nn.MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY_RLtrZMb76"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from collections import OrderedDict\n",
        "def compute_metrics(pred):\n",
        "    preds,labels = pred\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    # print(acc,precision,recall,f1)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "    # return {'accuracy':acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2gc12oO8sul"
      },
      "outputs": [],
      "source": [
        "def build_optimizer(model, config):\n",
        "    transformer_vars = [i for i in model.parameters()]\n",
        "    optimizer = torch.optim.AdamW(transformer_vars, lr=config.learning_rate)\n",
        "    num_train_steps = int(config.num_train_examples / config.batch_size * config.epochs)\n",
        "    num_warmup_steps = int(num_train_steps * config.warmup_proportion)\n",
        "\n",
        "    scheduler = get_constant_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = num_warmup_steps)\n",
        "      \n",
        "    return optimizer,scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qez2PxUpVt5y"
      },
      "source": [
        "## training methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHuurlIzScA3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "5AhZ_TQ_Tl39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWUDACNCScIu"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "def train_epoch(model, dataset, optimizers,config,epoch):\n",
        "  training_step_outputs=[]\n",
        "  # Measure how long the training epoch takes.\n",
        "  t0 = time.time()\n",
        "  print_each_n_step = 100\n",
        "  optimizer = optimizers[0]\n",
        "  scheduler= optimizers[1]\n",
        "  train_dl = dataset.train_dataloader()\n",
        "  test_dl = dataset.val_dataloader()\n",
        "  n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
        "  # Put the model into training mode.\n",
        "  model.train()\n",
        "  # For each batch of training data...\n",
        "  \n",
        "  for step, batch in enumerate(train_dl):\n",
        "    # Progress update every print_each_n_step batches.\n",
        "    if step % print_each_n_step == 0 and not step == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)    \n",
        "        # Report progress.\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dl), elapsed))\n",
        "\n",
        "    # Unpack this training batch from our dataloader. \n",
        "    b_input_ids = batch[0].to(config.device)\n",
        "    b_input_mask = batch[1].to(config.device)\n",
        "    b_labels = batch[2].to(config.device)\n",
        "    real_batch_size=b_input_ids.shape[0]\n",
        "    loss,logits,hidden_states = model(input_ids=b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "    preds = torch.argmax(logits,dim=1)\n",
        "    #---------------------------------\n",
        "    #  OPTIMIZATION\n",
        "    #---------------------------------\n",
        "    # Avoid gradient accumulation\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate weigth updates\n",
        "    # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "    loss.backward() \n",
        "    \n",
        "    #clip grad norm\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Apply modifications\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    metrics = {'train/loss_step':loss}\n",
        "    if step + 1 < n_steps_per_epoch:\n",
        "      wandb.log(metrics)\n",
        "    \n",
        "    training_step_outputs.append(OrderedDict({'loss':loss.detach().cpu(),\\\n",
        "                                              'preds':preds.detach().cpu(),\\\n",
        "                                              'labels':b_labels.detach().cpu(),\n",
        "                                              }))\n",
        "    \n",
        "  \n",
        "  all_losses = np.mean([x['loss'].item() for x in training_step_outputs])\n",
        "  all_preds = [y.item() for x in training_step_outputs for y in x['preds']]\n",
        "  all_labels =  [y.item() for x in training_step_outputs for y in x['labels']]\n",
        "  # result = compute_metrics((all_preds,all_labels))\n",
        "  result = classification_report(all_labels,all_preds,target_names =dataset.label_list,output_dict=True)\n",
        "  print(result)\n",
        "  accuracy = result.pop('accuracy')\n",
        "  wandb.log({'train/accuracy':accuracy})\n",
        "  new_result = dict()\n",
        "  \n",
        "  for k in result:\n",
        "    for keys in result[k]:\n",
        "      new_result[f'train/{k}_{keys}']=result[k][keys]\n",
        "  wandb.log(new_result)\n",
        "\n",
        "  training_time = format_time(time.time() - t0)                    \n",
        "  print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "  validation_metrics = validate_model(model,test_dl,config,epoch,dataset.label_list)\n",
        "  return validation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS56Vp1LScIu"
      },
      "outputs": [],
      "source": [
        "def validate_model(model,test_dl,config,epoch,label_list):\n",
        "  print(\"Running Test...\")\n",
        "  t0 = time.time()\n",
        "  val_step_outputs = []\n",
        "  nll_loss = torch.nn.CrossEntropyLoss()\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "\n",
        "  for batch in test_dl:\n",
        "    # Unpack this training batch from our dataloader. \n",
        "    b_input_ids = batch[0].to(config.device)\n",
        "    b_input_mask = batch[1].to(config.device)\n",
        "    b_labels = batch[2].to(config.device)\n",
        "    # print('taken input from batch')\n",
        "    # Tell pytorch not to bother with constructing the compute graph during\n",
        "    # the forward pass, since this is only needed for backprop (training).\n",
        "    with torch.no_grad():        \n",
        "      loss,logits,hidden_states = model(input_ids=b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "    \n",
        "    # Accumulate the test loss.\n",
        "    # print(filtered_logits.shape)\n",
        "    # print('run discriminator')\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    loss = nll_loss(logits, b_labels)\n",
        "    # print('calculated everythin')\n",
        "    wandb.log({'val/loss':loss})\n",
        "    # print('logged everythin')\n",
        "\n",
        "    val_step_outputs.append(OrderedDict({'loss':loss.detach().cpu(),\\\n",
        "                                          'preds':preds.detach().cpu(),'labels':b_labels.detach().cpu()}))\n",
        "  \n",
        "  all_losses = np.mean([x['loss'].item() for x in val_step_outputs])\n",
        "  \n",
        "  all_preds = [y.item() for x in val_step_outputs for y in x['preds']]\n",
        "  # print(all_preds)\n",
        "  all_labels =  [y.item() for x in val_step_outputs for y in x['labels']]\n",
        "  # print(all_labels)\n",
        "  # result = compute_metrics((all_preds,all_labels))\n",
        "  result = classification_report(all_labels,all_preds,target_names =label_list,output_dict=True)\n",
        "  print('val',result)\n",
        "  accuracy = result.pop('accuracy')\n",
        "  wandb.log({'val/accuracy':accuracy})\n",
        "  new_result = dict()\n",
        "  \n",
        "  for k in result:\n",
        "    for keys in result[k]:\n",
        "      new_result[f'val/{k}_{keys}']=result[k][keys]\n",
        "  wandb.log(new_result)\n",
        "  test_time = format_time(time.time() - t0)\n",
        "  print(\"  Test took: {:}\".format(test_time))\n",
        "  return new_result\n",
        "  # return val_step_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imzTarz5Vv1p"
      },
      "source": [
        "##train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z36NGhYOUMKs"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As3kaem_NVOW"
      },
      "outputs": [],
      "source": [
        "# !rm -r /content/wandb\n",
        "# !rm -r /content/lightning_logs\n",
        "!rm -r /content/category_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xu229RVUMKt"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/category_title\n",
        "!chmod 765 /content/category_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGSGQRoRUMKt"
      },
      "outputs": [],
      "source": [
        "best_test_f1 =0 \n",
        "seed_everything(42)\n",
        "# run = wandb.init(project=\"headline_category_detection\",entity='colab-team',config=sweep_config)\n",
        "run = wandb.init(config=sweep_config)\n",
        "config = wandb.config\n",
        "main_dataset = MNISTDataModule(model_number = config.model_number, label_list = label_list)\n",
        "clickbaitmodel = TransformerForSequenceClassification(\n",
        "    pretrained_model_name = all_checkpoints[config.model_number],num_classes = len(label_list))\n",
        "# wandb.watch(clickbaitmodel, log=\"gradients\")\n",
        "optimizers = build_optimizer(clickbaitmodel, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivm-Hc3pUMKt"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm(range(config.epochs)):\n",
        "    validation_metrics = train_epoch(clickbaitmodel,main_dataset,optimizers,config,epoch+1)\n",
        "    model_name = all_checkpoints[config.model_number].split('/')[-1]\n",
        "    # test_f1 = validation_metrics['val/epoch_f1']\n",
        "    PATH=f'/content/category_title/{model_name}_{epoch}.pt'\n",
        "    torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model': clickbaitmodel.state_dict(),\n",
        "          'optimizer_state_dict': optimizers[0].state_dict(),\n",
        "          }, PATH)\n",
        "    model_artifact = wandb.Artifact(\n",
        "          f\"{model_name}_{epoch}\", type=f'{model_name}',\n",
        "          description=f\"{model_name}_{epoch}\",\n",
        "          metadata=dict(config))\n",
        "    PATH=f'/content/category_title/banglabert_{epoch}.pt'\n",
        "    model_artifact.add_file(PATH)\n",
        "    run.log_artifact(model_artifact)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0hAjsa6yt5k"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## labelling category with best mdoel"
      ],
      "metadata": {
        "id": "L0pHcm7YZ_8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "ixTcj7_mm6DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init()\n",
        "artifact = run.use_artifact('colab-team/headline_category_detection/banglabert_7:v0', type='banglabert')\n",
        "artifact_dir = artifact.download()"
      ],
      "metadata": {
        "id": "SwkCWzoAYviS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "sweep_config = {\n",
        "    'learning_rate': 1e-05,\n",
        "      'batch_size':64,\n",
        "      'warmup_proportion':0.1,\n",
        "      'num_train_examples': 15056,\n",
        "      'model_number': 7,\n",
        "      'epochs': 10,\n",
        "     'device': 'cuda'}\n",
        "sweep_config"
      ],
      "metadata": {
        "id": "Cpz2RekVl1a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3_0FIZNjGzC"
      },
      "outputs": [],
      "source": [
        "best_test_f1 =0 \n",
        "seed_everything(42)\n",
        "# run = wandb.init(project=\"headline_category_detection\",entity='colab-team',config=sweep_config)\n",
        "run = wandb.init(config=sweep_config)\n",
        "config = wandb.config\n",
        "clickbaitmodel = TransformerForSequenceClassification(\n",
        "    pretrained_model_name = all_checkpoints[config.model_number],num_classes = len(label_list))\n",
        "optimizers = build_optimizer(clickbaitmodel, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgsceoJmgmdQ"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/artifacts/banglabert_7:v0/banglabert_7.pt\"\n",
        "checkpoint = torch.load(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJC95fSFfHNi"
      },
      "outputs": [],
      "source": [
        "clickbaitmodel.load_state_dict(checkpoint['model'])\n",
        "optimizer = optimizers[0]\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sYP9pJhfHOr"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/machine_learning/clickbait_identification/latestFixed.csv')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_dataset = pd.read_csv('/content/drive/MyDrive/machine_learning/clickbait_identification/combined.csv')\n",
        "unlabelled_dataset.head()"
      ],
      "metadata": {
        "id": "5dDa0AXoptoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_dataset.shape"
      ],
      "metadata": {
        "id": "vZIJH8Ij4EQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "input_mask_array = []\n",
        "model_number = 7\n",
        "tokenizer = eval(tokenizer_list[model_number])\n",
        "max_seq_length = 64"
      ],
      "metadata": {
        "id": "3VTpo4LFqCF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = list(dataset['cleaned_title'].values)\n",
        "# Tokenization \n",
        "for text in train_examples:\n",
        "  encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "  input_ids.append(encoded_sent)   \n",
        "# Attention to token (to ignore padded input wordpieces)\n",
        "for sent in input_ids:\n",
        "  att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "  input_mask_array.append(att_mask)\n",
        "# Convertion to Tensor\n",
        "input_ids = torch.tensor(input_ids) \n",
        "input_mask_array = torch.tensor(input_mask_array)\n",
        "label_id_array = torch.tensor([0]*dataset.shape[0], dtype=torch.long)\n",
        "tensordataset = TensorDataset(input_ids, input_mask_array,label_id_array)"
      ],
      "metadata": {
        "id": "nwf3VsMlgZ4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = list(unlabelled_dataset['cleaned_title'].values)\n",
        "# Tokenization \n",
        "for text in train_examples:\n",
        "  encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "  input_ids.append(encoded_sent)   \n",
        "# Attention to token (to ignore padded input wordpieces)\n",
        "for sent in input_ids:\n",
        "  att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "  input_mask_array.append(att_mask)\n",
        "# Convertion to Tensor\n",
        "input_ids = torch.tensor(input_ids) \n",
        "input_mask_array = torch.tensor(input_mask_array)\n",
        "label_id_array = torch.tensor([0]*unlabelled_dataset.shape[0], dtype=torch.long)\n",
        "tensordataset = TensorDataset(input_ids, input_mask_array,label_id_array)"
      ],
      "metadata": {
        "id": "-xabzRPFp7Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tensordataset)"
      ],
      "metadata": {
        "id": "164hzzqqsF9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = SequentialSampler\n",
        "datadl =  DataLoader(\n",
        "                tensordataset,  # The training samples.\n",
        "                sampler = sampler(tensordataset), \n",
        "                batch_size = 128,\n",
        "                pin_memory = True,\n",
        "                num_workers=4) "
      ],
      "metadata": {
        "id": "0h3exwsspFHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in datadl:\n",
        "  print(batch)\n",
        "  break"
      ],
      "metadata": {
        "id": "unW_Dlv-fS2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in datadl:\n",
        "  print(tokenizer.batch_decode(batch[0],skip_special_tokens=True))\n",
        "  # print(dataset.loc[:5,'cleaned_title'])\n",
        "  break"
      ],
      "metadata": {
        "id": "1jZmXXHXonF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model,test_dl,config,epoch,label_list):\n",
        "  print(\"Running Test...\")\n",
        "  t0 = time.time()\n",
        "  val_step_outputs = []\n",
        "  nll_loss = torch.nn.CrossEntropyLoss()\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "\n",
        "  for batch in test_dl:\n",
        "    # Unpack this training batch from our dataloader. \n",
        "    b_input_ids = batch[0].to(config.device)\n",
        "    b_input_mask = batch[1].to(config.device)\n",
        "    b_labels = batch[2].to(config.device)\n",
        "    # print('taken input from batch')\n",
        "    # Tell pytorch not to bother with constructing the compute graph during\n",
        "    # the forward pass, since this is only needed for backprop (training).\n",
        "    with torch.no_grad():        \n",
        "      loss,logits,hidden_states = model(input_ids=b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    loss = nll_loss(logits, b_labels)\n",
        "    val_step_outputs.append(OrderedDict({'loss':loss.detach().cpu(),\\\n",
        "                                          'preds':preds.detach().cpu(),'labels':b_labels.detach().cpu()}))\n",
        "  all_preds = [y.item() for x in val_step_outputs for y in x['preds']]\n",
        "  all_labels =  [y.item() for x in val_step_outputs for y in x['labels']]\n",
        "  test_time = format_time(time.time() - t0)\n",
        "  print(\"  Test took: {:}\".format(test_time))\n",
        "  # return new_result\n",
        "  # return val_step_outputs\n",
        "  return all_preds"
      ],
      "metadata": {
        "id": "Xw_hGL_EkILs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config,label_list"
      ],
      "metadata": {
        "id": "ilo-ZFxDkKBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = validate_model(clickbaitmodel,datadl,config,0,label_list)\n",
        "len(preds)"
      ],
      "metadata": {
        "id": "skhe0jvxkb2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVcPteqSfp5x"
      },
      "outputs": [],
      "source": [
        "preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['category'] = [label_list[i] for i in preds]\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "5HC1h8AMlO8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_dataset['category'] = [label_list[i] for i in preds]\n",
        "unlabelled_dataset.head()"
      ],
      "metadata": {
        "id": "BqHCll8fqgk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLIZ7OJogD7A"
      },
      "outputs": [],
      "source": [
        "dataset.to_csv('/content/drive/MyDrive/machine_learning/clickbait_identification/latestFixedCategory.csv',index=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(dataset.category.values)"
      ],
      "metadata": {
        "id": "Inid2DztmnbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(unlabelled_dataset.category.values)"
      ],
      "metadata": {
        "id": "flqJYnLat0Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabelled_dataset.to_csv('/content/drive/MyDrive/machine_learning/clickbait_identification/combined_category.csv',index=None)"
      ],
      "metadata": {
        "id": "kG5JouL5uDv8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}